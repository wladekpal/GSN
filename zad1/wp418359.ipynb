{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpointing\n",
        "\n",
        "Your task is to implement checkpointing for a MLP using NumPy.\n",
        "\n",
        "You are free to use the implementation of a MLP and the backpropagation algorithm that you have developed during lab sessions.\n",
        "\n",
        "The key takeaway from this task is that with checkpointing we can trade off the computational resources needed to compute the forward pass of the network for the memory requirement needed to perform a backward pass in the network, which is often a major bottleneck when training large networks. In plain english, we can slightly increase the time required for training our network to save some of our GPU's precious memory.\n",
        "\n",
        "## What is checkpointing?\n",
        "\n",
        "The aim of checkpointing is to save every $n$-th layer's (e.g. every 2-nd layer's) forward result (instead of saving every layer's forward result as in plain backpropagation) and use these checkpoints for recomputing the forward pass of the network upon doing a backward pass. Checkpoint layers are kept in memory after the forward pass, while the remaining activations are recomputed at most once. After being recomputed, the non-checkpoint layers are kept in memory until they are no longer required."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Q0hm4DvY9hIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What should be done\n",
        "\n",
        "1. Take the implementation a MLP trained with backpropagation. Analyze the algorithm with respect to the memory that is used by the algorithm with respect to the number of hidden layers.\n",
        "\n",
        "2. Implement a class NetworkWithCheckpointing that inherits from the Network class defined during lab sessions by:\n",
        "    a) implementing a method `forward_between_checkpoints` that will recompute the forward pass of the network using one of the checkpointed layers\n",
        "    b) override the method `backprop` to use only checkpointed layers and otherwise compute the activations using `forward_between_checkpoints` method and keep it in memory until no longer needed.\n",
        "\n",
        "3. Train your network with checkpoinintg on MNIST. Compare running times and memory usage with respect to the network without checkpointing.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d_LWjvU29hID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Checkpointing for a MLP"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7LJobi7J9hIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "p2Itr-Tc9hIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-20 16:31:42--  https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.64.254, 52.217.202.128, 54.231.202.0, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.64.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11490434 (11M) [application/octet-stream]\n",
            "Saving to: ‘mnist.npz’\n",
            "\n",
            "mnist.npz           100%[===================>]  10.96M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-11-20 16:31:43 (95.1 MB/s) - ‘mnist.npz’ saved [11490434/11490434]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ],
      "metadata": {
        "id": "I-Gunjrt9hIF",
        "outputId": "3ed90045-cf3b-45fa-c6f4-10e407f7597a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "outputs": [],
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "def load_mnist(path='mnist.npz'):\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ],
      "metadata": {
        "id": "cOuFnnnK9hIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))"
      ],
      "metadata": {
        "id": "P2SNgB0i9_Da"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This version of backpropagation was taken from google colab notebook for lab 3. The only change is the addition of ability to turn off the logging in SGD function."
      ],
      "metadata": {
        "id": "-A1f02EUZjSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "outputs": [],
      "source": [
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a batch\n",
        "        a = a.T\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.matmul(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
        "        # eta is the learning rate      \n",
        "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n",
        "            \n",
        "        self.weights = [w-(eta/len(mini_batch[0]))*nw \n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch[0]))*nb \n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    def backprop(self, x, y):\n",
        "        # For a single input (x,y) return a pair of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "        g = x\n",
        "        gs = [g] # list to store all the gs, layer by layer\n",
        "        fs = [] # list to store all the fs, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            f = np.dot(w, g)+b\n",
        "            fs.append(f)\n",
        "            g = sigmoid(f)\n",
        "            gs.append(g)\n",
        "        # backward pass <- both steps at once\n",
        "        dLdg = self.cost_derivative(gs[-1], y)\n",
        "        dLdfs = []\n",
        "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
        "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
        "            dLdfs.append(dLdf)\n",
        "            dLdg = np.matmul(w.T, dLdf)\n",
        "        \n",
        "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])] \n",
        "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)] \n",
        "        return (dLdBs,dLdWs)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
        "        corr = np.argmax(test_data[1],axis=1).T\n",
        "        return np.mean(pred==corr)\n",
        "    \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y) \n",
        "    \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, log=True):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
        "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
        "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n",
        "            if test_data and log:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
        "            elif log:\n",
        "                print(\"Epoch: {0}\".format(j))\n"
      ],
      "metadata": {
        "id": "TQvBVvDG9hIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory analysis\n",
        "Standard backpropagation algorithm stores all activations computed during the forward pass. This version of the algorithm is vectorized, meaning that in single call to backpropagation whole mini batch of data is processed. This means that the activations of all layers will take \n",
        "$$O\\left(\\text{minibatch_size} * ∑_{i=1} s_i\\right)$$\n",
        "memory (where $s_i$ is the size of layer). In other words it is the sum of sizes of all layers, except the first one, times the size of the mini batch. This means that, memory depends on both the number of layers and the size of each layer."
      ],
      "metadata": {
        "id": "68w8FpNgWF4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpointing implementation"
      ],
      "metadata": {
        "id": "Sy-e3wUeWzBj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "outputs": [],
      "source": [
        "class NetworkWithCheckpointing(Network):\n",
        "    def __init__(self, sizes, checkpoint_every_nth_layer: int = 0, *args, **kwargs):\n",
        "        super().__init__(sizes, *args, **kwargs)\n",
        "        self.checkpoint_every_nth_layer = checkpoint_every_nth_layer\n",
        "\n",
        "    def forward_between_checkpoints(self, a, checkpoint_idx_start, layer_idx_end):\n",
        "        after_activation = [a]\n",
        "\n",
        "        for layer_idx in range(checkpoint_idx_start, layer_idx_end):\n",
        "          layer_product = self.weights[layer_idx] @ after_activation[-1] + self.biases[layer_idx]\n",
        "          layer_product_sig = sigmoid(layer_product)\n",
        "\n",
        "          after_activation.append(layer_product_sig)\n",
        "\n",
        "        return after_activation\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        # if self.checkpoint_every_nth_layer is equal to 0 solution falls back to backward solution implemented in super class.\n",
        "        if self.checkpoint_every_nth_layer == 0:\n",
        "          return super().backprop(x, y)\n",
        "\n",
        "        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n",
        "        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n",
        "\n",
        "        g = x\n",
        "        # Activations list stores activations on checkpoints\n",
        "        activations = [g] \n",
        "\n",
        "        for layer_idx in range(self.num_layers - 1):\n",
        "          f = np.dot(self.weights[layer_idx], g) + self.biases[layer_idx]\n",
        "          g = sigmoid(f)\n",
        "\n",
        "          # We append activation to list only every self.checkpoint_every_nth_layer\n",
        "          if (layer_idx + 1) % self.checkpoint_every_nth_layer == 0:\n",
        "            activations.append(g)\n",
        "\n",
        "      \n",
        "        # Preparation to checkpointed backpropagation\n",
        "        prev_checkpoint_number = -1\n",
        "        cur_checkpoint_idx = self.num_layers - 1\n",
        "        prev_checkpoint_idx = self.num_layers - ((self.num_layers - 1) % self.checkpoint_every_nth_layer) - 1\n",
        "        position_in_chunk = -1\n",
        "\n",
        "        # Activations between last checkpoint and end of network are restored\n",
        "        restored_chunk = self.forward_between_checkpoints(activations[prev_checkpoint_number], prev_checkpoint_idx, cur_checkpoint_idx - 1)\n",
        "        # Last activation of the network doesn't need to restored, because it is still remembered from forward pass.\n",
        "        restored_chunk.append(g)\n",
        "        dLdg = self.cost_derivative(g, y)\n",
        "\n",
        "        # Backpropagating over network layers\n",
        "        for layer_idx in range(1, self.num_layers):\n",
        "          # If current layer is one of the checkpointed ones activations between it and previous checkpoint are restored.\n",
        "          if (self.num_layers - layer_idx) % self.checkpoint_every_nth_layer == 0:\n",
        "            prev_checkpoint_number -= 1\n",
        "            cur_checkpoint_idx = prev_checkpoint_idx - 1\n",
        "            prev_checkpoint_idx -= self.checkpoint_every_nth_layer\n",
        "            position_in_chunk = -1\n",
        "\n",
        "            restored_chunk = self.forward_between_checkpoints(activations[prev_checkpoint_number], prev_checkpoint_idx, cur_checkpoint_idx)\n",
        "            # Last layer doesn't need to be computed because it is checkpointed.\n",
        "            restored_chunk.append(activations[prev_checkpoint_number + 1])\n",
        "            \n",
        "\n",
        "          # This part is very similar to standard backpropagation,\n",
        "          # instead of global list of activations there is a list of activations between current checkpoints.\n",
        "\n",
        "          # Last layer of activation can deleted from activation list, as it will not be used later.\n",
        "          g = restored_chunk.pop()\n",
        "          \n",
        "          dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
        "          dLdg = np.matmul(self.weights[-layer_idx].T, dLdf)\n",
        "\n",
        "          delta_nabla_w[-layer_idx] = np.matmul(dLdf, restored_chunk[-1].T)\n",
        "          delta_nabla_b[-layer_idx] = np.sum(dLdf, axis=1).reshape(-1, 1)\n",
        "          \n",
        "          position_in_chunk -= 1\n",
        "        \n",
        "        return (delta_nabla_b, delta_nabla_w)"
      ],
      "metadata": {
        "id": "qZrsoGqI9hIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory analysis\n",
        "In this version of backpropagation algorithm we only save every $n$-th activation instead of every single one during the forward pass. Then, during the backward pass missing activations are computed from saved ones. Because of this less memory is used, but running time is longer. for forward pass only every nth layer is saved, so memory usage is \n",
        "$$O\\left(\\sum_{i=1}s_{xi}\\right)$$\n",
        "where $x$ is the `checkpoint_every_nth_layer` paramter. Then during the backward pass, between every two adjacent checkpoints activations are restored using\n",
        "$$O\\left(\\sum_{i=c_j}^{c_{j+1}}s_{i}\\right)$$\n",
        "memory (where $c_j$ and $c_{j+1}$ are indexes of checkpoints. After backpropagation crosses checkpoint corresponding activations can be deleted. So the peak memory usage during backward pass is\n",
        "$$O\\left(\\max_{j}\\sum_{i=c_j}^{c_{j+1}}s_{i}\\right)$$\n",
        "Depending on the shape of the network and number of layers this can be significantly less than in the backward implementation without checkpointing."
      ],
      "metadata": {
        "id": "nTwpJRcPY43K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy comparison\n",
        "To check, whether the implementation is correct we can run 4 networks (one standard and three with different number of checkpoints) with the same initial set of weights. Since checkpointing should not change the way backpropagation algorithm works the accuracies in all 4 networks should be identical"
      ],
      "metadata": {
        "id": "NmMZVrfne8Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [784, 30, 30, 30, 30, 10]\n",
        "epochs = 5\n",
        "\n",
        "network = Network(architecture)\n",
        "weights = network.weights\n",
        "biases = network.biases\n",
        "print(\"Standard network without checkpointing:\")\n",
        "network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 1)\n",
        "network.weights = weights\n",
        "network.biases = biases\n",
        "print(\"Checkpointing every layer:\")\n",
        "network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 2)\n",
        "network.weights = weights\n",
        "network.biases = biases\n",
        "print(\"Checkpointing every second layer:\")\n",
        "network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 4)\n",
        "network.weights = weights\n",
        "network.biases = biases\n",
        "print(\"Checkpointing every 4 layers:\")\n",
        "network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3UyrvYpdgEo",
        "outputId": "1d749498-273a-438b-dd4d-4c7cdcc4ea62"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard network without checkpointing:\n",
            "Epoch: 0, Accuracy: 0.7747\n",
            "Epoch: 1, Accuracy: 0.8523\n",
            "Epoch: 2, Accuracy: 0.8748\n",
            "Epoch: 3, Accuracy: 0.8903\n",
            "Epoch: 4, Accuracy: 0.9025\n",
            "Checkpointing every layer:\n",
            "Epoch: 0, Accuracy: 0.7747\n",
            "Epoch: 1, Accuracy: 0.8523\n",
            "Epoch: 2, Accuracy: 0.8748\n",
            "Epoch: 3, Accuracy: 0.8903\n",
            "Epoch: 4, Accuracy: 0.9025\n",
            "Checkpointing every second layer:\n",
            "Epoch: 0, Accuracy: 0.7747\n",
            "Epoch: 1, Accuracy: 0.8523\n",
            "Epoch: 2, Accuracy: 0.8748\n",
            "Epoch: 3, Accuracy: 0.8903\n",
            "Epoch: 4, Accuracy: 0.9025\n",
            "Checkpointing every 4 layers:\n",
            "Epoch: 0, Accuracy: 0.7747\n",
            "Epoch: 1, Accuracy: 0.8523\n",
            "Epoch: 2, Accuracy: 0.8748\n",
            "Epoch: 3, Accuracy: 0.8903\n",
            "Epoch: 4, Accuracy: 0.9025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running time comparison\n",
        "For the sake of readability of the output logging of accuracy is disabled during running time comparison."
      ],
      "metadata": {
        "id": "h9IbIpkwN1Gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on simple, small architecture. \n",
        "Standard implementation and checkpointing every layer should have similar times (small time differencese can be present due to slight differneces in implementation). Since the network has only one hidden layer, there should be no significant difference between checkpoint every 2 and 4 layers (or more)."
      ],
      "metadata": {
        "id": "QdPnBG9RfwCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [784, 100, 10]\n",
        "epochs = 10\n",
        "\n",
        "network = Network(architecture)\n",
        "print(\"Standard network without checkpointing:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 1)\n",
        "print(\"Checkpointing every layer:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 2)\n",
        "print(\"Checkpointing every second layer:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 4)\n",
        "print(\"Checkpointing every 4 layers:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcd3pODZM8YC",
        "outputId": "99f0ce6b-2990-4a77-82f7-afb444daf159"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard network without checkpointing:\n",
            "14.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every layer:\n",
            "15.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every second layer:\n",
            "20.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every 4 layers:\n",
            "21 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on longer network\n",
        "Again there sould be no significan difference between the standard network and network with checkpoint every layer. On network of this size difference in time between checkpoint every second and every fourth layer should start to show.\n",
        "\n"
      ],
      "metadata": {
        "id": "HupwAYA8RbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [784, 30, 30, 30, 30, 30, 30, 10]\n",
        "epochs = 10\n",
        "\n",
        "network = Network(architecture)\n",
        "print(\"Standard network without checkpointing:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 1)\n",
        "print(\"Checkpointing every layer:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 2)\n",
        "print(\"Checkpointing every second layer:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 4)\n",
        "print(\"Checkpointing every 4 layers:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6sE8staRZfD",
        "outputId": "d0f45aff-54df-4411-b002-cf13ad650bf8"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard network without checkpointing:\n",
            "10.8 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every layer:\n",
            "11.8 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every second layer:\n",
            "15.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every 4 layers:\n",
            "16.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test on very long network\n"
      ],
      "metadata": {
        "id": "pNVaLybZiwAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [784] + ([30] * 50) + [10]\n",
        "epochs = 10\n",
        "\n",
        "network = Network(architecture)\n",
        "print(\"Standard network without checkpointing:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 1)\n",
        "print(\"Checkpointing every layer:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 2)\n",
        "print(\"Checkpointing every second layer:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 4)\n",
        "print(\"Checkpointing every 4 layers:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "\n",
        "network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = 10)\n",
        "print(\"Checkpointing every 10 layers:\")\n",
        "%timeit -n1 -r1 network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "460B27yDWv4S",
        "outputId": "0bdfcb49-33c7-4d15-cd6c-dd716d085877"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard network without checkpointing:\n",
            "58.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every layer:\n",
            "1min 3s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every second layer:\n",
            "1min 16s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every 4 layers:\n",
            "1min 22s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
            "Checkpointing every 10 layers:\n",
            "1min 26s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory usage\n",
        "Using `tracemalloc` we can compare the usage of memory of different networks. Since checkpointing is used mostly to allow bigger network to fit on devices with smaller memory what we are most interested in is the peak memory usage, and that is the statistics that we will compare here."
      ],
      "metadata": {
        "id": "Omji7xmhOATX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tracemalloc\n",
        "\n",
        "def check_memory(architecture, checkpoint_every_nth_layer):\n",
        "  tracemalloc.start()\n",
        "  if checkpoint_every_nth_layer == 0:\n",
        "    network = Network(architecture)\n",
        "  else:\n",
        "    network = NetworkWithCheckpointing(architecture, checkpoint_every_nth_layer = checkpoint_every_nth_layer)\n",
        "\n",
        "  network.SGD((x_train, y_train), epochs=epochs, mini_batch_size=100, eta=3., test_data=(x_test, y_test), log=False)\n",
        "  peak = tracemalloc.get_traced_memory()[1]\n",
        "  tracemalloc.stop()\n",
        "  return peak"
      ],
      "metadata": {
        "id": "qtJSx3x47rL_"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small network\n",
        "When comparing memory usage on small network the difference in memory isn't very big, but still can be seen."
      ],
      "metadata": {
        "id": "Z67EUZgQ4Qi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [784, 30, 30, 30, 30, 30, 10]\n",
        "epochs = 2\n",
        "\n",
        "\n",
        "standard_peak = check_memory(architecture, 0)\n",
        "print(\"Standard network memory usage peak: \", standard_peak)\n",
        "\n",
        "every_2_peak = check_memory(architecture, 2)\n",
        "print(\"Checkpoint every 2 layers memory usage peak: \", every_2_peak)\n",
        "\n",
        "every_4_peak = check_memory(architecture, 4)\n",
        "print(\"Checkpoint every 4 layers memory usage peak: \", every_4_peak)\n",
        "\n",
        "every_10_peak = check_memory(architecture, 10)\n",
        "print(\"Checkpoint every 10 layers memory usage peak: \", every_10_peak)\n",
        "\n",
        "print(\"Memory saving when using checkpoint every 2 layers:\", standard_peak - every_2_peak)\n",
        "print(\"Memory saving when using checkpoint every 4 layers:\", standard_peak - every_4_peak)\n",
        "print(\"Memory saving when using checkpoint every 10 layers:\", standard_peak - every_10_peak)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dYVX4-r3iY4",
        "outputId": "c52699f1-9cee-4b2a-fb7e-e3402fc0e806"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard network memory usage peak:  1557491\n",
            "Checkpoint every 2 layers memory usage peak:  1408902\n",
            "Checkpoint every 4 layers memory usage peak:  1372008\n",
            "Checkpoint every 10 layers memory usage peak:  1347150\n",
            "Memory saving when using checkpoint every 2 layers: 148589\n",
            "Memory saving when using checkpoint every 4 layers: 185483\n",
            "Memory saving when using checkpoint every 10 layers: 210341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigger network\n",
        "Memory usage differences can be seen better on bigger networks, like one below (network is very 'long' and 'thin' to allow for faster working).\n",
        "Checkpointing does improve memory usage, although the difference between standard network and checkpoints every second layer is significantly bigger than that between checkpoint every second layer and checkpoint every fourth layer."
      ],
      "metadata": {
        "id": "OIyxZf064ehb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "architecture = [784] + ([30] * 50) + [10]\n",
        "epochs = 2\n",
        "\n",
        "\n",
        "standard_peak = check_memory(architecture, 0)\n",
        "print(\"Standard network memory usage peak: \", standard_peak)\n",
        "\n",
        "every_2_peak = check_memory(architecture, 2)\n",
        "print(\"Checkpoint every 2 layers memory usage peak: \", every_2_peak)\n",
        "\n",
        "every_4_peak = check_memory(architecture, 4)\n",
        "print(\"Checkpoint every 4 layers memory usage peak: \", every_4_peak)\n",
        "\n",
        "every_10_peak = check_memory(architecture, 10)\n",
        "print(\"Checkpoint every 10 layers memory usage peak: \", every_10_peak)\n",
        "\n",
        "print(\"Memory saving when using checkpoint every 2 layers:\", standard_peak - every_2_peak)\n",
        "print(\"Memory saving when using checkpoint every 4 layers:\", standard_peak - every_4_peak)\n",
        "print(\"Memory saving when using checkpoint every 10 layers:\", standard_peak - every_10_peak)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lntEfPlCkBh",
        "outputId": "db6e1767-621e-4aec-94d8-3e922bc36f5d"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard network memory usage peak:  6012939\n",
            "Checkpoint every 2 layers memory usage peak:  2660051\n",
            "Checkpoint every 4 layers memory usage peak:  2347372\n",
            "Checkpoint every 10 layers memory usage peak:  2177360\n",
            "Memory saving when using checkpoint every 2 layers: 3352888\n",
            "Memory saving when using checkpoint every 4 layers: 3665567\n",
            "Memory saving when using checkpoint every 10 layers: 3835579\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q0hm4DvY9hIA",
        "d_LWjvU29hID"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}